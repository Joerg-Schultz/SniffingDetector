{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "`pip install tensorflow`\n",
    "`pip install tensorflow_io` (can't be installed via conda)\n",
    "`pip install mlflow`\n",
    "Do I get a problem with numpy? Tensorflow uses 1.23, mlflo installed 1.24?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.layers import Conv2D, Dense, Flatten, MaxPooling2D, Dropout\n",
    "import tensorflow_io as tfio\n",
    "import os\n",
    "import pathlib\n",
    "\n",
    "#import numpy as np\n",
    "import tensorflow.experimental.numpy as tnp\n",
    "from tensorflow.python.ops import gen_audio_ops as audio_ops\n",
    "\n",
    "#  enable NumPy behavior for TensorFlow:\n",
    "tnp.experimental_enable_numpy_behavior()\n",
    "\n",
    "import mlflow"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "sniffingDir = \"data/train/Sniffing\"\n",
    "backgroundDir = \"data/train/Background\"\n",
    "testSniffingDir = \"data/test/Sniffing\"\n",
    "testBackgroundDir = \"data/test/Background\"\n",
    "\n",
    "validation_fraction = 0.2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def generate_spectrogram(file_path, label):\n",
    "    # does not scale as sniffing should be the same independent of background level\n",
    "    audio_tensor = tfio.audio.AudioIOTensor(file_path, dtype=tf.int16)\n",
    "    audio = tf.cast(audio_tensor[:], tf.float32)\n",
    "    spectrogram = audio_ops.audio_spectrogram(audio,\n",
    "                                              window_size=320,\n",
    "                                              stride=160,\n",
    "                                              magnitude_squared=True)\n",
    "    spectrogram = tf.nn.pool(\n",
    "        input=tf.expand_dims(spectrogram, -1),\n",
    "        window_shape=[1, 6],\n",
    "        strides=[1, 6],\n",
    "        pooling_type='AVG',\n",
    "        padding='SAME')\n",
    "    spectrogram = tf.squeeze(spectrogram, axis=0)\n",
    "    # Not sure whether the log is a good idea...\n",
    "    spectrogram = tnp.log10(spectrogram + 1e-6)\n",
    "    return spectrogram, label\n",
    "\n",
    "def prepare_data(dir, value):\n",
    "    filePath = os.path.join(dir, \"*.wav\")\n",
    "    files = tf.data.Dataset.list_files(filePath)\n",
    "    values = tf.zeros(len(files)) if value == 0 else tf.ones(len(files))\n",
    "    data = tf.data.Dataset.zip((files, tf.data.Dataset.from_tensor_slices(values)))\n",
    "    spectrogramData = data.map(generate_spectrogram)\n",
    "    return spectrogramData\n",
    "\n",
    "def generate_binary_dataset(trueDataDir, falseDataDir):\n",
    "    trueData = prepare_data(trueDataDir, 1)\n",
    "    falseData = prepare_data(falseDataDir, 0)\n",
    "    combinedData = trueData.concatenate(falseData)\n",
    "    combinedData = combinedData.cache()\n",
    "    return combinedData.shuffle(buffer_size=combinedData.cardinality().numpy())\n",
    "\n",
    "def split_validation(allData, fraction):\n",
    "    numberDataSets = allData.cardinality().numpy()\n",
    "    validationCount = round(numberDataSets * fraction)\n",
    "    trainSet = allData.take(numberDataSets - validationCount)\n",
    "    validationSet = allData.skip(numberDataSets - validationCount).take(validationCount)\n",
    "    return trainSet, validationSet\n",
    "\n",
    "def batch_prefetch(dataSet):\n",
    "    dataSet = dataSet.batch(16)\n",
    "    dataSet = dataSet.prefetch(8)\n",
    "    return dataSet\n",
    "\n",
    "def calc_accuracy(predictions, true_values):\n",
    "    accuracy = sum(map(lambda x, y: x == y == 1, true_values, predictions))/sum(true_values)\n",
    "    return accuracy\n",
    "\n",
    "def representative_data_gen():\n",
    "    repr_samples, repr_labels = train.as_numpy_iterator().next()\n",
    "    yield [repr_samples]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputShape (99, 43, 1)\n"
     ]
    }
   ],
   "source": [
    "data = generate_binary_dataset(trueDataDir=sniffingDir, falseDataDir=backgroundDir)\n",
    "train, validation = split_validation(data, validation_fraction)\n",
    "train = batch_prefetch(train)\n",
    "validation = batch_prefetch(validation)\n",
    "samples, labels = train.as_numpy_iterator().next()\n",
    "inputShape = samples.shape[1:]\n",
    "print(f\"inputShape {inputShape}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv_layer1 (Conv2D)        (None, 99, 43, 4)         40        \n",
      "                                                                 \n",
      " max_pooling1 (MaxPooling2D)  (None, 49, 21, 4)        0         \n",
      "                                                                 \n",
      " conv_layer2 (Conv2D)        (None, 49, 21, 4)         148       \n",
      "                                                                 \n",
      " max_pooling2 (MaxPooling2D)  (None, 24, 10, 4)        0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 960)               0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 960)               0         \n",
      "                                                                 \n",
      " hidden_layer1 (Dense)       (None, 40)                38440     \n",
      "                                                                 \n",
      " output (Dense)              (None, 1)                 41        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 38,669\n",
      "Trainable params: 38,669\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/atomic14/diy-alexa/blob/master/model/Train%20Model.ipynb\n",
    "model = Sequential([\n",
    "    Conv2D(4, 3,\n",
    "           padding='same',\n",
    "           activation='relu',\n",
    "           kernel_regularizer=regularizers.l2(0.001),\n",
    "           name='conv_layer1',\n",
    "           #input_shape=(IMG_WIDTH, IMG_HEIGHT, 1)),\n",
    "           input_shape=inputShape),\n",
    "    MaxPooling2D(name='max_pooling1', pool_size=(2,2)),\n",
    "    Conv2D(4, 3,\n",
    "           padding='same',\n",
    "           activation='relu',\n",
    "           kernel_regularizer=regularizers.l2(0.001),\n",
    "           name='conv_layer2'),\n",
    "    MaxPooling2D(name='max_pooling2', pool_size=(2,2)),\n",
    "    Flatten(),\n",
    "    Dropout(0.2),\n",
    "    Dense(\n",
    "        40,\n",
    "        activation='relu',\n",
    "        kernel_regularizer=regularizers.l2(0.001),\n",
    "        name='hidden_layer1'\n",
    "    ),\n",
    "    Dense(\n",
    "        1,\n",
    "        activation='sigmoid',\n",
    "        kernel_regularizer=regularizers.l2(0.001),\n",
    "        name='output'\n",
    "    )\n",
    "])\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "              metrics=[[tf.keras.metrics.Recall(),tf.keras.metrics.Precision()]])\n",
    "\n",
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.7910 - recall: 0.0000e+00 - precision: 0.0000e+00 - val_loss: 0.9048 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00\n",
      "Epoch 2/30\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.8164 - recall: 1.0000 - precision: 0.3333 - val_loss: 1.0724 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00\n",
      "Epoch 3/30\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.9042 - recall: 1.0000 - precision: 0.3333 - val_loss: 0.5738 - val_recall: 1.0000 - val_precision: 1.0000\n",
      "Epoch 4/30\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.6279 - recall: 1.0000 - precision: 0.6667 - val_loss: 0.6238 - val_recall: 1.0000 - val_precision: 1.0000\n",
      "Epoch 5/30\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.7372 - recall: 0.5000 - precision: 1.0000 - val_loss: 0.6128 - val_recall: 1.0000 - val_precision: 1.0000\n",
      "Epoch 6/30\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.5954 - recall: 1.0000 - precision: 1.0000 - val_loss: 0.5945 - val_recall: 1.0000 - val_precision: 1.0000\n",
      "Epoch 7/30\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.7725 - recall: 0.0000e+00 - precision: 0.0000e+00 - val_loss: 0.7523 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00\n",
      "Epoch 8/30\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.6451 - recall: 1.0000 - precision: 1.0000 - val_loss: 0.7478 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00\n",
      "Epoch 9/30\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.7060 - recall: 1.0000 - precision: 1.0000 - val_loss: 0.9013 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00\n",
      "Epoch 10/30\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.6501 - recall: 1.0000 - precision: 1.0000 - val_loss: 0.7846 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00\n",
      "Epoch 11/30\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.5848 - recall: 1.0000 - precision: 0.6667 - val_loss: 0.8492 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00\n",
      "Epoch 12/30\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.7799 - recall: 1.0000 - precision: 0.3333 - val_loss: 0.5707 - val_recall: 1.0000 - val_precision: 1.0000\n",
      "Epoch 13/30\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.6012 - recall: 1.0000 - precision: 1.0000 - val_loss: 0.7333 - val_recall: 1.0000 - val_precision: 1.0000\n",
      "Epoch 14/30\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.6421 - recall: 1.0000 - precision: 1.0000 - val_loss: 0.9310 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00\n",
      "Epoch 15/30\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.7045 - recall: 0.0000e+00 - precision: 0.0000e+00 - val_loss: 0.9447 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00\n",
      "Epoch 16/30\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.4753 - recall: 0.0000e+00 - precision: 0.0000e+00 - val_loss: 0.9682 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00\n",
      "Epoch 17/30\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.4925 - recall: 1.0000 - precision: 1.0000 - val_loss: 0.9965 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00\n",
      "Epoch 18/30\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.8148 - recall: 0.0000e+00 - precision: 0.0000e+00 - val_loss: 0.3765 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00\n",
      "Epoch 19/30\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.5597 - recall: 0.5000 - precision: 1.0000 - val_loss: 0.4745 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00\n",
      "Epoch 20/30\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.7641 - recall: 0.0000e+00 - precision: 0.0000e+00 - val_loss: 0.7751 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00\n",
      "Epoch 21/30\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.6854 - recall: 1.0000 - precision: 1.0000 - val_loss: 0.9403 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00\n",
      "Epoch 22/30\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.6946 - recall: 1.0000 - precision: 0.5000 - val_loss: 0.8834 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00\n",
      "Epoch 23/30\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.5036 - recall: 1.0000 - precision: 0.6667 - val_loss: 1.0387 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00\n",
      "Epoch 24/30\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.6755 - recall: 1.0000 - precision: 0.3333 - val_loss: 0.9316 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00\n",
      "Epoch 25/30\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.7233 - recall: 1.0000 - precision: 0.3333 - val_loss: 0.4596 - val_recall: 1.0000 - val_precision: 1.0000\n",
      "Epoch 26/30\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.5086 - recall: 1.0000 - precision: 1.0000 - val_loss: 0.5869 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00\n",
      "Epoch 27/30\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.5398 - recall: 1.0000 - precision: 1.0000 - val_loss: 0.4587 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00\n",
      "Epoch 28/30\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.6143 - recall: 1.0000 - precision: 1.0000 - val_loss: 0.4077 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00\n",
      "Epoch 29/30\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.9122 - recall: 0.0000e+00 - precision: 0.0000e+00 - val_loss: 0.7461 - val_recall: 1.0000 - val_precision: 1.0000\n",
      "Epoch 30/30\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.6237 - recall: 0.5000 - precision: 1.0000 - val_loss: 0.6377 - val_recall: 1.0000 - val_precision: 1.0000\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(train, epochs=30, validation_data=validation)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Expected 'tf.Tensor(False, shape=(), dtype=bool)' to be true. Summarized data: b'No files matched pattern: data/test/Sniffing\\\\*.wav'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mInvalidArgumentError\u001B[0m                      Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[11], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m testData \u001B[38;5;241m=\u001B[39m \u001B[43mgenerate_binary_dataset\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrueDataDir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtestSniffingDir\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfalseDataDir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtestBackgroundDir\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      2\u001B[0m testData \u001B[38;5;241m=\u001B[39m batch_prefetch(testData)\n\u001B[0;32m      3\u001B[0m list_predictions \u001B[38;5;241m=\u001B[39m []\n",
      "Cell \u001B[1;32mIn[7], line 29\u001B[0m, in \u001B[0;36mgenerate_binary_dataset\u001B[1;34m(trueDataDir, falseDataDir)\u001B[0m\n\u001B[0;32m     28\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mgenerate_binary_dataset\u001B[39m(trueDataDir, falseDataDir):\n\u001B[1;32m---> 29\u001B[0m     trueData \u001B[38;5;241m=\u001B[39m \u001B[43mprepare_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrueDataDir\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     30\u001B[0m     falseData \u001B[38;5;241m=\u001B[39m prepare_data(falseDataDir, \u001B[38;5;241m0\u001B[39m)\n\u001B[0;32m     31\u001B[0m     combinedData \u001B[38;5;241m=\u001B[39m trueData\u001B[38;5;241m.\u001B[39mconcatenate(falseData)\n",
      "Cell \u001B[1;32mIn[7], line 22\u001B[0m, in \u001B[0;36mprepare_data\u001B[1;34m(dir, value)\u001B[0m\n\u001B[0;32m     20\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mprepare_data\u001B[39m(\u001B[38;5;28mdir\u001B[39m, value):\n\u001B[0;32m     21\u001B[0m     filePath \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(\u001B[38;5;28mdir\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m*.wav\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m---> 22\u001B[0m     files \u001B[38;5;241m=\u001B[39m \u001B[43mtf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdata\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mDataset\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlist_files\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilePath\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     23\u001B[0m     values \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mzeros(\u001B[38;5;28mlen\u001B[39m(files)) \u001B[38;5;28;01mif\u001B[39;00m value \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m tf\u001B[38;5;241m.\u001B[39mones(\u001B[38;5;28mlen\u001B[39m(files))\n\u001B[0;32m     24\u001B[0m     data \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mdata\u001B[38;5;241m.\u001B[39mDataset\u001B[38;5;241m.\u001B[39mzip((files, tf\u001B[38;5;241m.\u001B[39mdata\u001B[38;5;241m.\u001B[39mDataset\u001B[38;5;241m.\u001B[39mfrom_tensor_slices(values)))\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\NeuralNetwork\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:1471\u001B[0m, in \u001B[0;36mDatasetV2.list_files\u001B[1;34m(file_pattern, shuffle, seed, name)\u001B[0m\n\u001B[0;32m   1464\u001B[0m condition \u001B[38;5;241m=\u001B[39m math_ops\u001B[38;5;241m.\u001B[39mgreater(array_ops\u001B[38;5;241m.\u001B[39mshape(matching_files)[\u001B[38;5;241m0\u001B[39m], \u001B[38;5;241m0\u001B[39m,\n\u001B[0;32m   1465\u001B[0m                              name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmatch_not_empty\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m   1467\u001B[0m message \u001B[38;5;241m=\u001B[39m math_ops\u001B[38;5;241m.\u001B[39madd(\n\u001B[0;32m   1468\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNo files matched pattern: \u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   1469\u001B[0m     string_ops\u001B[38;5;241m.\u001B[39mreduce_join(file_pattern, separator\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m\"\u001B[39m), name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmessage\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m-> 1471\u001B[0m assert_not_empty \u001B[38;5;241m=\u001B[39m \u001B[43mcontrol_flow_ops\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mAssert\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1472\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcondition\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[43mmessage\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msummarize\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mname\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43massert_not_empty\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1473\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m ops\u001B[38;5;241m.\u001B[39mcontrol_dependencies([assert_not_empty]):\n\u001B[0;32m   1474\u001B[0m   matching_files \u001B[38;5;241m=\u001B[39m array_ops\u001B[38;5;241m.\u001B[39midentity(matching_files)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\NeuralNetwork\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    151\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    152\u001B[0m   filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[1;32m--> 153\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[0;32m    154\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m    155\u001B[0m   \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\NeuralNetwork\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py:156\u001B[0m, in \u001B[0;36mAssert\u001B[1;34m(condition, data, summarize, name)\u001B[0m\n\u001B[0;32m    154\u001B[0m     xs \u001B[38;5;241m=\u001B[39m ops\u001B[38;5;241m.\u001B[39mconvert_n_to_tensor(data)\n\u001B[0;32m    155\u001B[0m     data_str \u001B[38;5;241m=\u001B[39m [_summarize_eager(x, summarize) \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m xs]\n\u001B[1;32m--> 156\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m errors\u001B[38;5;241m.\u001B[39mInvalidArgumentError(\n\u001B[0;32m    157\u001B[0m         node_def\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m    158\u001B[0m         op\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m    159\u001B[0m         message\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mExpected \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m to be true. Summarized data: \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m\n\u001B[0;32m    160\u001B[0m         (condition, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(data_str)))\n\u001B[0;32m    161\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m\n\u001B[0;32m    163\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m ops\u001B[38;5;241m.\u001B[39mname_scope(name, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAssert\u001B[39m\u001B[38;5;124m\"\u001B[39m, [condition, data]) \u001B[38;5;28;01mas\u001B[39;00m name:\n",
      "\u001B[1;31mInvalidArgumentError\u001B[0m: Expected 'tf.Tensor(False, shape=(), dtype=bool)' to be true. Summarized data: b'No files matched pattern: data/test/Sniffing\\\\*.wav'"
     ]
    }
   ],
   "source": [
    "testData = generate_binary_dataset(trueDataDir=testSniffingDir, falseDataDir=testBackgroundDir)\n",
    "testData = batch_prefetch(testData)\n",
    "list_predictions = []\n",
    "list_true = []\n",
    "for i in range(testData.cardinality().numpy()):\n",
    "    X_test, y_test = testData.as_numpy_iterator().next()\n",
    "    testResult = map(lambda x: 1 if x > 0.5 else 0, model.predict(X_test))\n",
    "    list_predictions.extend(testResult)\n",
    "    list_true.extend(y_test)\n",
    "calc_accuracy(list_predictions,list_true)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _update_step_xla while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\Joerg\\Documents\\tmp\\SniffingDetector\\Detecting\\NeuralNetwork\\sniffing_model/1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\Joerg\\Documents\\tmp\\SniffingDetector\\Detecting\\NeuralNetwork\\sniffing_model/1/assets\n"
     ]
    },
    {
     "data": {
      "text/plain": "43400"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "savedModelDir = os.path.join(os.getcwd(), \"sniffing_model/1/\")\n",
    "tf.saved_model.save(model, savedModelDir)\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(savedModelDir)\n",
    "\n",
    "# Post Training Quantization\n",
    "# https://www.tensorflow.org/lite/performance/post_training_quantization\n",
    "# Integer with float fallback (using default float input/output)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_data_gen\n",
    "\n",
    "# Integer only\n",
    "# doesn't work here, as I have float32 input and want an output 0<= x <= 1\n",
    "# atomic14_converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "# atomic14_converter.inference_input_type = tf.int8  # or tf.uint8\n",
    "# atomic14_converter.inference_output_type = tf.int8\n",
    "\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "tflite_model_file = pathlib.Path('model.tflite')\n",
    "tflite_model_file.write_bytes(tflite_model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'testData' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[13], line 7\u001B[0m\n\u001B[0;32m      5\u001B[0m tinyResults \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m      6\u001B[0m tinyExpected \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m----> 7\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[43mtestData\u001B[49m\u001B[38;5;241m.\u001B[39mcardinality()\u001B[38;5;241m.\u001B[39mnumpy()):\n\u001B[0;32m      8\u001B[0m     X_test, y_test \u001B[38;5;241m=\u001B[39m testData\u001B[38;5;241m.\u001B[39mas_numpy_iterator()\u001B[38;5;241m.\u001B[39mnext()\n\u001B[0;32m      9\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m j \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(X_test)):\n",
      "\u001B[1;31mNameError\u001B[0m: name 'testData' is not defined"
     ]
    }
   ],
   "source": [
    "interpreter = tf.lite.Interpreter(model_content=tflite_model)\n",
    "interpreter.allocate_tensors()\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "tinyResults = []\n",
    "tinyExpected = []\n",
    "for i in range(testData.cardinality().numpy()):\n",
    "    X_test, y_test = testData.as_numpy_iterator().next()\n",
    "    for j in range(len(X_test)):\n",
    "        interpreter.set_tensor(input_details[0]['index'], [X_test[j]])\n",
    "        interpreter.invoke()\n",
    "        tflite_result = interpreter.get_tensor(output_details[0]['index'])\n",
    "        tinyResults.append(1 if tflite_result[0][0] > 0.5 else 0)\n",
    "        tinyExpected.append(y_test[j])\n",
    "\n",
    "tinyAccuracy = calc_accuracy(tinyResults, tinyExpected)\n",
    "print(tinyAccuracy)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Run `xxd -i model.tflite > model_data.cc` in git bash"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}